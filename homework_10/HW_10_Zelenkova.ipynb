{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative  Matrix  Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 11314 raw text documents\n"
     ]
    }
   ],
   "source": [
    "raw_documents = []\n",
    "snippets = []\n",
    "for text in newsgroups.data:\n",
    "    raw_documents.append( text.lower() )\n",
    "    snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 350 entries\n"
     ]
    }
   ],
   "source": [
    "# custom stopwords\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as f:\n",
    "    for line in f.readlines():\n",
    "        custom_stop_words.append( line.strip().lower() )\n",
    "        \n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11314 X 8889 TF-IDF-normalized document-term matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['articles-tfidf.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create BoW + tf-idf model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "joblib.dump((A,terms,snippets), \"articles-tfidf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edu (476.15)\n",
      "com (343.46)\n",
      "subject (213.89)\n",
      "lines (212.11)\n",
      "organization (209.69)\n",
      "writes (193.92)\n",
      "article (191.88)\n",
      "university (183.79)\n",
      "posting (167.86)\n",
      "host (160.93)\n",
      "nntp (159.33)\n",
      "ca (151.82)\n",
      "know (151.48)\n",
      "people (150.03)\n",
      "get (143.27)\n",
      "cs (131.41)\n",
      "think (131.30)\n",
      "good (120.71)\n",
      "time (114.49)\n",
      "distribution (113.65)\n"
     ]
    }
   ],
   "source": [
    "# top features from tf-idf model\n",
    "import operator\n",
    "\n",
    "\n",
    "sums = np.array(A.sum(axis=0)).ravel()\n",
    "# map weights to the terms\n",
    "weights = { term: sums[col] for col, term in enumerate(terms)}\n",
    "ranking = sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%s (%.2f)\" % ( pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(A,terms,snippets) = joblib.load( \"articles-tfidf.pkl\")\n",
    "\n",
    "# create the model\n",
    "# k = 20\n",
    "# model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "\n",
    "# W = model.fit_transform( A )\n",
    "# H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer_tf = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,  0.01,\n",
       "        0.01,  0.01,  0.01,  0.03,  0.01,  0.01,  0.01,  0.01,  0.01,\n",
       "        0.01,  0.83])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8889)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25.59,   0.05,   0.05,   0.05,   0.05,   0.3 ,   0.05,   0.05,\n",
       "         0.05,   0.05,   0.05,   0.05,   0.05,   0.08,   0.05,   0.05,\n",
       "         0.05,   0.05,   0.05,   0.05])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[:,terms.index('geb')].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: com, netcom, hp, ibm, article, writes, sun, organization, subject, lines, att, stratus, posting, nntp, distribution, host, reply, corp, austin, mot\n",
      "Topic 02: god, jesus, bible, christian, christians, christ, faith, believe, people, church, christianity, truth, life, hell, religion, sin, rutgers, heaven, man, think\n",
      "Topic 03: edu, university, cs, posting, nntp, host, organization, subject, lines, article, cmu, writes, uiuc, washington, andrew, distribution, thanks, please, berkeley, mit\n",
      "Topic 04: windows, window, file, dos, files, program, card, graphics, mouse, help, problem, screen, version, thanks, video, color, drivers, de, pc, system\n",
      "Topic 05: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, surrender, pittsburgh, edu, univ, science, soon, computer, reply\n",
      "Topic 06: key, clipper, chip, encryption, keys, escrow, government, algorithm, security, crypto, secure, nsa, system, secret, privacy, des, public, law, encrypted, wiretap\n",
      "Topic 07: team, game, games, players, hockey, season, play, nhl, win, baseball, teams, league, player, toronto, detroit, go, leafs, runs, wings, roger\n",
      "Topic 08: nasa, gov, space, larc, jpl, jsc, gsfc, henry, center, research, moon, orbit, baalke, shuttle, ___, alaska, __, launch, fnal, lerc\n",
      "Topic 09: people, gun, car, think, get, time, good, right, guns, know, going, see, government, way, really, want, things, back, thing, something\n",
      "Topic 10: scsi, drive, ide, controller, drives, disk, hard, bus, mac, card, floppy, pc, apple, system, hd, isa, cd, motherboard, ram, sale\n",
      "Topic 11: uk, ac, dcs, __, cam, ed, university, 44, mathew, ___, liverpool, subject, mantis, lines, demon, organization, nz, newsreader, mail, edinburgh\n",
      "Topic 12: keith, caltech, sgi, livesey, morality, wpd, solntze, cco, jon, objective, schneider, moral, allan, atheists, pasadena, system, edu, institute, technology, california\n",
      "Topic 13: israel, israeli, jews, arab, arabs, jewish, lebanese, jake, peace, israelis, policy, lebanon, adam, cpr, palestinian, org, palestinians, land, igc, palestine\n",
      "Topic 14: armenian, turkish, armenians, armenia, argic, serdar, turks, turkey, genocide, zuma, soviet, sera, uucp, greek, people, sdpa, muslim, azerbaijan, 1920, extermination\n",
      "Topic 15: ohio, state, magnus, acs, edu, university, cis, ryan, magnusug, drugs, scharfy, __, rscharfy, oil, article, bottom, nntp, host, posting, _____\n",
      "Topic 16: cleveland, cwru, freenet, ins, reserve, western, edu, po, case, usa, oh, hela, host, tony, nntp, previous, posting, ohio, university, ch981\n",
      "Topic 17: virginia, cramer, optilink, clayton, gay, men, homosexual, sexual, male, clas, sex, homosexuals, study, greg, uunet, people, promiscuous, consent, pyramid, partners\n",
      "Topic 18: ca, canada, bnr, bc, uwo, ontario, university, carleton, sfu, ottawa, ubc, organization, subject, lines, cs, writes, article, fraser, maynard, bike\n",
      "Topic 19: access, digex, pat, net, express, online, communications, prb, usa, greenbelt, md, com, steve, posting, nntp, host, unix, public, organization, subject\n",
      "Topic 20: columbia, cc, gld, cunixb, gary, dare, edu, utexas, cunixa, buffalo, jets, ccwf, domi, bitnet, cunixc, go, souviens, phds, keenan, je\n"
     ]
    }
   ],
   "source": [
    "# show topic descriptors\n",
    "def get_descriptor(terms, H, topic_index, top):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. From: bmdelane@midway.uchicago.edu (brian manning delaney)\n",
      "Subject: RESULT: sci.life-extension passe\n",
      "02. From: k4bnc@cbnewsh.cb.att.com (john.a.siegel)\n",
      "Subject: Can't set COM4\n",
      "Organization: AT&T\n",
      "Distributi\n",
      "03. From: tomk@skywalker.bocaraton.ibm.com (Thomas Chun-Hong Kok)\n",
      "Subject: Re: MOOLIT and OLIT\n",
      "Organizat\n",
      "04. From: cdt@sw.stratus.com (C. D. Tavares)\n",
      "Subject: Re: Ax the ATF\n",
      "Organization: Stratus Computer, Inc\n",
      "05. From: dana@lando.la.locus.com (Dana H. Myers)\n",
      "Subject: What is a squid? (was Re: Riceburner Respect)\n",
      "06. From: essbaum@rchland.vnet.ibm.com (Alexander Essbaum)\n",
      "Subject: Re: ++BIKE SOLD OVER NET 600 MILES A\n",
      "07. From: channui@austin.ibm.com (Christopher Chan-Nui)\n",
      "Subject: Re: Two pointing devices in one COM-por\n",
      "08. From: rosen@kranz.enet.dec.com (Jim Rosenkranz)\n",
      "Subject: Re: Metal powder,steel,iron.\n",
      "Reply-To: rose\n",
      "09. From: vinlai@cbnewsb.cb.att.com (vincent.lai)\n",
      "Subject: Third party car antennas ...\n",
      "Organization: AT\n",
      "10. From: slagle@lmsc.lockheed.com (Mark Slagle)\n",
      "Subject: Re: NRA Fucks Up Bigtime\n",
      "Reply-To: slagle@lmsc\n",
      "11. From: jec@watson.ibm.com\n",
      "Subject: Contraceptive pill\n",
      "Reply-To: jec@zurich.ibm.com\n",
      "Disclaimer: This p\n",
      "12. From: rhorwell@crab.atc.boeing.com (Roland Faragher-Horwell,crab)\n",
      "Subject: Re: What is \" Volvo \" ?\n",
      "R\n",
      "13. From: swkirch@sun6850.nrl.navy.mil (Steve Kirchoefer)\n",
      "Subject: 3rd CFV and VOTE ACK: misc.health.dia\n",
      "14. From: richg@sequent.com (Richard Garrett)\n",
      "Subject: Wanted original Shanghai for PC\n",
      "Article-I.D.: seq\n",
      "15. From: cdt@sw.stratus.com (C. D. Tavares)\n",
      "Subject: Re: HR 1276 (\"A gun law I can live with!\" :-)\n",
      "Orga\n",
      "16. From: sylvain@netcom.com (Nicholas Sylvain)\n",
      "Subject: Re: \"Proper gun control?\" What is proper gun co\n",
      "17.  cs.utexas.edu!geraldo.cc.utexas.edu!portal.austin.ibm.com!awdprime.austin.ibm.com!karner\n",
      "Subject: R\n",
      "18. From: behanna@syl.nj.nec.com (Chris BeHanna)\n",
      "Subject: Re: What is a squid? (was Re: Riceburner Respe\n",
      "19. From: cdt@sw.stratus.com (C. D. Tavares)\n",
      "Subject: A Scoop of Waco Road, Please\n",
      "Organization: Stratus\n",
      "20. From: mulvey@blurt.oswego.edu (Allen Mulvey, SUNY, Oswego, NY)\n",
      "Subject: Re: Can't set COM4\n",
      "Distribut\n"
     ]
    }
   ],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets\n",
    "\n",
    "topic_snippets = get_top_snippets( snippets, W, 0, 20 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-model-nmf-k20.pkl']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((W,H,terms,snippets), \"articles-model-nmf-k%02d.pkl\" % k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:12<00:00, 15.73s/it]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 27\n",
    "\n",
    "topic_models = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 12267 terms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens\n",
    "            \n",
    "\n",
    "docgen = TokenGenerator( raw_documents, custom_stop_words )\n",
    "w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)\n",
    "print( \"Model has %d terms\" % len(w2v_model.wv.vocab) )\n",
    "w2v_model.save(\"w2v-model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate topic coherence TC-W2C as a mean pairwise similarity of words from topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.2602\n",
      "K=05: Coherence=0.3121\n",
      "K=06: Coherence=0.3457\n",
      "K=07: Coherence=0.3595\n",
      "K=08: Coherence=0.3585\n",
      "K=09: Coherence=0.3602\n",
      "K=10: Coherence=0.3726\n",
      "K=11: Coherence=0.3758\n",
      "K=12: Coherence=0.3821\n",
      "K=13: Coherence=0.4034\n",
      "K=14: Coherence=0.3943\n",
      "K=15: Coherence=0.4017\n",
      "K=16: Coherence=0.4077\n",
      "K=17: Coherence=0.4042\n",
      "K=18: Coherence=0.4099\n",
      "K=19: Coherence=0.4090\n",
      "K=20: Coherence=0.4175\n",
      "K=21: Coherence=0.4156\n",
      "K=22: Coherence=0.4166\n",
      "K=23: Coherence=0.4183\n",
      "K=24: Coherence=0.4170\n",
      "K=25: Coherence=0.4221\n",
      "K=26: Coherence=0.4142\n",
      "K=27: Coherence=0.4159\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "inverse = defaultdict(set)\n",
    "for i, text in enumerate(raw_documents):\n",
    "    spl = tokenizer_tf(text)\n",
    "    for word in spl:\n",
    "        inverse[word].add(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # def PMI(w1, w2):\n",
    "# w1 = 'israeli'\n",
    "# w2 = 'policy'\n",
    "N_docs = 11314\n",
    "eps = 10**(-12)\n",
    "def PMI(eps,w1,w2): #for 2 words\n",
    "\n",
    "    n_of_both_occ = len(inverse[w1]&inverse[w2])\n",
    "    n_w1 = len(inverse[w1])\n",
    "    n_w2 =len(inverse[w2])\n",
    "    p_w1 = n_w1/N_docs\n",
    "    p_w2= n_w2/N_docs\n",
    "    p_w1w2 = n_of_both_occ/N_docs\n",
    "    pmi = np.log((p_w1w2+ eps) / (p_w1*p_w2))\n",
    "#     print(pmi)\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PMI(eps, 'word', 'religion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=20\n",
    "def calculate_coherence_2(k, term_rankings):\n",
    "    overall_coherence = 0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(PMI(eps, pair[0], pair[1]))\n",
    "#             print(pair_scores)\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = (2 / (N*(N-1)))*sum(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.5660\n",
      "K=05: Coherence=1.0084\n",
      "K=06: Coherence=1.2218\n",
      "K=07: Coherence=1.2777\n",
      "K=08: Coherence=1.2073\n",
      "K=09: Coherence=1.2485\n",
      "K=10: Coherence=1.2925\n",
      "K=11: Coherence=1.1710\n",
      "K=12: Coherence=1.2735\n",
      "K=13: Coherence=1.4813\n",
      "K=14: Coherence=1.3544\n",
      "K=15: Coherence=1.3418\n",
      "K=16: Coherence=1.2012\n",
      "K=17: Coherence=1.2863\n",
      "K=18: Coherence=1.3451\n",
      "K=19: Coherence=1.2422\n",
      "K=20: Coherence=1.3030\n",
      "K=21: Coherence=1.2243\n",
      "K=22: Coherence=1.2488\n",
      "K=23: Coherence=1.3117\n",
      "K=24: Coherence=1.3211\n",
      "K=25: Coherence=1.1111\n",
      "K=26: Coherence=1.2350\n",
      "K=27: Coherence=1.2481\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append(get_descriptor( terms, H, topic_index, 20))\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence_2(k, term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate UMass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_of_probs(eps,w1,w2): #for 2 words\n",
    "    n_of_both_occ = len(inverse[w1]&inverse[w2])\n",
    "    n_w1 = len(inverse[w1])\n",
    "    n_w2 =len(inverse[w2])\n",
    "    p_w1 = n_w1/N_docs\n",
    "    p_w2= n_w2/N_docs\n",
    "    p_w1w2 = n_of_both_occ/N_docs\n",
    "    if n_w2 > n_w2:\n",
    "        log = np.log((p_w1w2+ eps) / (p_w2))\n",
    "    else:\n",
    "        log = np.log((p_w1w2+ eps) / (p_w1))\n",
    "    return log\n",
    "\n",
    "N=20\n",
    "def calculate_coherence_3(k, term_rankings):\n",
    "#     print(N)\n",
    "    overall_coherence = 0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(log_of_probs(eps, pair[0], pair[1]))\n",
    "#             print(log_of_probs(eps, pair[0], pair[1]))\n",
    "#             print(pair_scores)\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = (2 / (N*(N-1)))*sum(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "#         print(overall_coherence)\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-1.8877\n",
      "K=05: Coherence=-1.7028\n",
      "K=06: Coherence=-1.6600\n",
      "K=07: Coherence=-1.7040\n",
      "K=08: Coherence=-1.8751\n",
      "K=09: Coherence=-1.9149\n",
      "K=10: Coherence=-1.9412\n",
      "K=11: Coherence=-2.0786\n",
      "K=12: Coherence=-1.9617\n",
      "K=13: Coherence=-1.8816\n",
      "K=14: Coherence=-1.9164\n",
      "K=15: Coherence=-1.9913\n",
      "K=16: Coherence=-2.2139\n",
      "K=17: Coherence=-2.0457\n",
      "K=18: Coherence=-1.9799\n",
      "K=19: Coherence=-2.1113\n",
      "K=20: Coherence=-2.1566\n",
      "K=21: Coherence=-2.3137\n",
      "K=22: Coherence=-2.1902\n",
      "K=23: Coherence=-2.1527\n",
      "K=24: Coherence=-2.1918\n",
      "K=25: Coherence=-2.4945\n",
      "K=26: Coherence=-2.2721\n",
      "K=27: Coherence=-2.2655\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence\n",
    "    k_values.append( k )\n",
    "    coherences.append(calculate_coherence_3(k,term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A,terms,snippets = joblib.load(\"articles-tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [12:22<00:00, 32.85s/it]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 25\n",
    "\n",
    "topic_models_LDA = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=1)\n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    topic_models_LDA.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.2889\n",
      "K=05: Coherence=0.2940\n",
      "K=06: Coherence=0.2924\n",
      "K=07: Coherence=0.2810\n",
      "K=08: Coherence=0.2980\n",
      "K=09: Coherence=0.2899\n",
      "K=10: Coherence=0.3175\n",
      "K=11: Coherence=0.3091\n",
      "K=12: Coherence=0.3064\n",
      "K=13: Coherence=0.3341\n",
      "K=14: Coherence=0.3131\n",
      "K=15: Coherence=0.3392\n",
      "K=16: Coherence=0.3429\n",
      "K=17: Coherence=0.3614\n",
      "K=18: Coherence=0.3452\n",
      "K=19: Coherence=0.3330\n",
      "K=20: Coherence=0.3659\n",
      "K=21: Coherence=0.3848\n",
      "K=22: Coherence=0.3750\n",
      "K=23: Coherence=0.3888\n",
      "K=24: Coherence=0.3590\n",
      "K=25: Coherence=0.3982\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-0.4645\n",
      "K=05: Coherence=-2.2140\n",
      "K=06: Coherence=-2.1885\n",
      "K=07: Coherence=-1.7606\n",
      "K=08: Coherence=-1.3123\n",
      "K=09: Coherence=-1.4677\n",
      "K=10: Coherence=-1.6211\n",
      "K=11: Coherence=-2.0313\n",
      "K=12: Coherence=-1.5209\n",
      "K=13: Coherence=-1.0770\n",
      "K=14: Coherence=-2.2817\n",
      "K=15: Coherence=-3.0259\n",
      "K=16: Coherence=-2.9542\n",
      "K=17: Coherence=-4.1425\n",
      "K=18: Coherence=-2.2705\n",
      "K=19: Coherence=-3.0868\n",
      "K=20: Coherence=-3.0116\n",
      "K=21: Coherence=-3.2068\n",
      "K=22: Coherence=-4.1823\n",
      "K=23: Coherence=-4.1239\n",
      "K=24: Coherence=-4.5041\n",
      "K=25: Coherence=-4.4365\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append(get_descriptor( terms, H, topic_index, 20))\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence_2(k, term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-2.3943\n",
      "K=05: Coherence=-4.8831\n",
      "K=06: Coherence=-4.9652\n",
      "K=07: Coherence=-4.3926\n",
      "K=08: Coherence=-4.0870\n",
      "K=09: Coherence=-4.2706\n",
      "K=10: Coherence=-4.5308\n",
      "K=11: Coherence=-4.9858\n",
      "K=12: Coherence=-4.4749\n",
      "K=13: Coherence=-4.1959\n",
      "K=14: Coherence=-5.4487\n",
      "K=15: Coherence=-6.6292\n",
      "K=16: Coherence=-6.6080\n",
      "K=17: Coherence=-8.0624\n",
      "K=18: Coherence=-5.7084\n",
      "K=19: Coherence=-6.8624\n",
      "K=20: Coherence=-6.8123\n",
      "K=21: Coherence=-7.0802\n",
      "K=22: Coherence=-8.3505\n",
      "K=23: Coherence=-8.4650\n",
      "K=24: Coherence=-8.6532\n",
      "K=25: Coherence=-8.7034\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence\n",
    "    k_values.append( k )\n",
    "    coherences.append(calculate_coherence_3(k,term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 25\n",
    "\n",
    "topic_models_LSA = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model = TruncatedSVD(n_components=k ) \n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    topic_models_LSA.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC-W2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.3417\n",
      "K=05: Coherence=0.3710\n",
      "K=06: Coherence=0.3590\n",
      "K=07: Coherence=0.3381\n",
      "K=08: Coherence=0.3645\n",
      "K=09: Coherence=0.3659\n",
      "K=10: Coherence=0.3564\n",
      "K=11: Coherence=0.3391\n",
      "K=12: Coherence=0.3636\n",
      "K=13: Coherence=0.3531\n",
      "K=14: Coherence=0.3839\n",
      "K=15: Coherence=0.3654\n",
      "K=16: Coherence=0.3606\n",
      "K=17: Coherence=0.3590\n",
      "K=18: Coherence=0.3562\n",
      "K=19: Coherence=0.3508\n",
      "K=20: Coherence=0.3731\n",
      "K=21: Coherence=0.3621\n",
      "K=22: Coherence=0.3478\n",
      "K=23: Coherence=0.3518\n",
      "K=24: Coherence=0.3643\n",
      "K=25: Coherence=0.3503\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-0.0296\n",
      "K=05: Coherence=0.1723\n",
      "K=06: Coherence=0.0790\n",
      "K=07: Coherence=-0.3688\n",
      "K=08: Coherence=0.3128\n",
      "K=09: Coherence=-0.2880\n",
      "K=10: Coherence=-0.2323\n",
      "K=11: Coherence=-0.1361\n",
      "K=12: Coherence=-0.0836\n",
      "K=13: Coherence=-0.5638\n",
      "K=14: Coherence=-0.0680\n",
      "K=15: Coherence=-0.6136\n",
      "K=16: Coherence=-0.4266\n",
      "K=17: Coherence=-0.4078\n",
      "K=18: Coherence=-0.5192\n",
      "K=19: Coherence=-0.9936\n",
      "K=20: Coherence=-0.5855\n",
      "K=21: Coherence=-0.7810\n",
      "K=22: Coherence=-0.9532\n",
      "K=23: Coherence=-0.8845\n",
      "K=24: Coherence=-0.8733\n",
      "K=25: Coherence=-0.8894\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append(get_descriptor( terms, H, topic_index, 20))\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence_2(k, term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-2.8086\n",
      "K=05: Coherence=-2.7923\n",
      "K=06: Coherence=-2.8982\n",
      "K=07: Coherence=-3.4353\n",
      "K=08: Coherence=-2.8520\n",
      "K=09: Coherence=-3.4670\n",
      "K=10: Coherence=-3.3958\n",
      "K=11: Coherence=-3.4566\n",
      "K=12: Coherence=-3.3997\n",
      "K=13: Coherence=-3.8663\n",
      "K=14: Coherence=-3.5777\n",
      "K=15: Coherence=-4.0955\n",
      "K=16: Coherence=-3.9557\n",
      "K=17: Coherence=-3.9231\n",
      "K=18: Coherence=-4.0124\n",
      "K=19: Coherence=-4.5190\n",
      "K=20: Coherence=-4.1324\n",
      "K=21: Coherence=-4.2777\n",
      "K=22: Coherence=-4.4550\n",
      "K=23: Coherence=-4.4478\n",
      "K=24: Coherence=-4.4444\n",
      "K=25: Coherence=-4.4463\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence\n",
    "    k_values.append( k )\n",
    "    coherences.append(calculate_coherence_3(k,term_rankings) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose 3 best models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC: K=25: Coherence=0.4221\n",
    "\n",
    "UCI: K=13: Coherence=1.4813\n",
    "\n",
    "Umass: K=06: Coherence=-1.6600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC: K=25: Coherence=0.3982\n",
    "\n",
    "UCI: K=04: Coherence=-0.4645\n",
    "\n",
    "Umass: K=05: Coherence=-2.7923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC: K=14: Coherence=0.3839\n",
    "        \n",
    "UCI: K=08: Coherence=0.3128\n",
    "\n",
    "Umass: K=05: Coherence=-2.7923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь выбираем из них самую хорошую по метрике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC: NMF\n",
    "\n",
    "UCI: NMF\n",
    "\n",
    "Umass: NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь посмотрим на дескрипторы у NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Topic 01: com, article, writes, netcom, hp, nasa, ibm, car, organization, sun, subject, lines, gov, ca, access, posting, nntp, distribution, host, digex\n",
      "Topic 02: god, people, jesus, think, believe, bible, christian, christians, life, faith, see, christ, jews, know, religion, israel, church, time, way, truth\n",
      "Topic 03: edu, university, posting, host, nntp, article, cs, organization, writes, subject, lines, cc, state, cwru, ohio, game, cleveland, team, cmu, ca\n",
      "Topic 04: windows, drive, card, dos, uk, scsi, file, ac, thanks, window, pc, help, problem, system, files, program, video, disk, graphics, mac\n",
      "Topic 05: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, edu, surrender, pittsburgh, science, univ, soon, computer, reply\n",
      "Topic 06: key, clipper, chip, encryption, keys, escrow, government, security, algorithm, crypto, system, public, nsa, secure, secret, privacy, law, des, enforcement, wiretap\n",
      "________________\n",
      "13\n",
      "Topic 01: com, article, writes, netcom, hp, nasa, ibm, car, organization, sun, subject, lines, gov, ca, access, posting, nntp, distribution, host, digex\n",
      "Topic 02: god, people, jesus, think, believe, bible, christian, christians, life, faith, see, christ, jews, know, religion, israel, church, time, way, truth\n",
      "Topic 03: edu, university, posting, host, nntp, article, cs, organization, writes, subject, lines, cc, state, cwru, ohio, game, cleveland, team, cmu, ca\n",
      "Topic 04: windows, drive, card, dos, uk, scsi, file, ac, thanks, window, pc, help, problem, system, files, program, video, disk, graphics, mac\n",
      "Topic 05: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, edu, surrender, pittsburgh, science, univ, soon, computer, reply\n",
      "Topic 06: key, clipper, chip, encryption, keys, escrow, government, security, algorithm, crypto, system, public, nsa, secure, secret, privacy, law, des, enforcement, wiretap\n",
      "Topic 07: com, netcom, hp, article, writes, ibm, car, sun, organization, subject, lines, posting, stratus, att, distribution, nntp, access, host, usa, get\n",
      "Topic 08: god, jesus, people, bible, christian, christians, believe, faith, christ, think, church, life, christianity, truth, hell, religion, see, know, man, sin\n",
      "Topic 09: edu, university, posting, host, nntp, cc, state, organization, article, cs, subject, lines, ohio, writes, cwru, cleveland, usa, distribution, cmu, uiuc\n",
      "Topic 10: windows, window, file, dos, files, program, card, graphics, help, thanks, mouse, version, problem, screen, video, de, color, drivers, software, system\n",
      "Topic 11: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, surrender, edu, pittsburgh, science, univ, soon, computer, reply\n",
      "Topic 12: key, clipper, chip, encryption, keys, escrow, government, algorithm, security, crypto, system, nsa, public, secure, law, secret, privacy, des, enforcement, encrypted\n",
      "Topic 13: ca, team, game, hockey, players, games, play, nhl, season, toronto, win, go, teams, player, league, baseball, canada, think, roger, good\n",
      "________________\n",
      "25\n",
      "Topic 01: com, article, writes, netcom, hp, nasa, ibm, car, organization, sun, subject, lines, gov, ca, access, posting, nntp, distribution, host, digex\n",
      "Topic 02: god, people, jesus, think, believe, bible, christian, christians, life, faith, see, christ, jews, know, religion, israel, church, time, way, truth\n",
      "Topic 03: edu, university, posting, host, nntp, article, cs, organization, writes, subject, lines, cc, state, cwru, ohio, game, cleveland, team, cmu, ca\n",
      "Topic 04: windows, drive, card, dos, uk, scsi, file, ac, thanks, window, pc, help, problem, system, files, program, video, disk, graphics, mac\n",
      "Topic 05: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, edu, surrender, pittsburgh, science, univ, soon, computer, reply\n",
      "Topic 06: key, clipper, chip, encryption, keys, escrow, government, security, algorithm, crypto, system, public, nsa, secure, secret, privacy, law, des, enforcement, wiretap\n",
      "Topic 07: com, netcom, hp, article, writes, ibm, car, sun, organization, subject, lines, posting, stratus, att, distribution, nntp, access, host, usa, get\n",
      "Topic 08: god, jesus, people, bible, christian, christians, believe, faith, christ, think, church, life, christianity, truth, hell, religion, see, know, man, sin\n",
      "Topic 09: edu, university, posting, host, nntp, cc, state, organization, article, cs, subject, lines, ohio, writes, cwru, cleveland, usa, distribution, cmu, uiuc\n",
      "Topic 10: windows, window, file, dos, files, program, card, graphics, help, thanks, mouse, version, problem, screen, video, de, color, drivers, software, system\n",
      "Topic 11: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, surrender, edu, pittsburgh, science, univ, soon, computer, reply\n",
      "Topic 12: key, clipper, chip, encryption, keys, escrow, government, algorithm, security, crypto, system, nsa, public, secure, law, secret, privacy, des, enforcement, encrypted\n",
      "Topic 13: ca, team, game, hockey, players, games, play, nhl, season, toronto, win, go, teams, player, league, baseball, canada, think, roger, good\n",
      "Topic 14: nasa, gov, space, larc, jpl, henry, moon, jsc, alaska, gsfc, center, research, orbit, shuttle, access, digex, ___, launch, baalke, toronto\n",
      "Topic 15: armenian, turkish, armenians, armenia, argic, serdar, turks, turkey, people, genocide, zuma, soviet, sera, uucp, greek, government, sdpa, azerbaijan, muslim, 1920\n",
      "Topic 16: scsi, drive, ide, controller, drives, disk, hard, bus, mac, card, floppy, pc, apple, system, hd, isa, cd, motherboard, ram, mb\n",
      "Topic 17: uk, ac, __, dcs, university, cam, ed, ___, 44, subject, lines, organization, mathew, liverpool, mantis, demon, nz, newsreader, mail, please\n",
      "Topic 18: keith, caltech, sgi, livesey, morality, wpd, cco, solntze, jon, objective, schneider, moral, allan, atheists, pasadena, edu, system, institute, technology, california\n",
      "Topic 19: israel, israeli, jews, arab, arabs, jewish, people, lebanese, peace, jake, israelis, policy, lebanon, adam, cpr, rights, war, palestinian, land, org\n",
      "Topic 20: com, netcom, hp, ibm, article, writes, sun, organization, subject, lines, posting, att, stratus, nntp, host, reply, corp, distribution, austin, 408\n",
      "Topic 21: god, jesus, bible, christians, christian, faith, believe, christ, people, church, life, truth, christianity, hell, religion, think, sin, heaven, man, rutgers\n",
      "Topic 22: edu, university, posting, host, nntp, cs, cwru, article, cleveland, organization, subject, lines, writes, washington, uiuc, berkeley, usa, distribution, freenet, colorado\n",
      "Topic 23: windows, dos, file, files, program, nt, os, version, run, win, system, ini, help, microsoft, running, disk, mouse, pc, ftp, driver\n",
      "Topic 24: pitt, geb, gordon, banks, cs, cadre, dsl, n3jxp, chastity, shameful, skepticism, intellect, surrender, edu, pittsburgh, univ, science, soon, computer, reply\n",
      "Topic 25: key, clipper, chip, encryption, keys, escrow, government, algorithm, security, crypto, secure, system, nsa, secret, des, privacy, public, encrypted, wiretap, phone\n",
      "________________\n"
     ]
    }
   ],
   "source": [
    "topic_choose = []\n",
    "for k in [6,13,25]:\n",
    "    model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_\n",
    "    topic_choose.append( (k,W,H) )\n",
    "\n",
    "descriptors = []\n",
    "for (k,W,H) in topic_choose:\n",
    "    print(k)\n",
    "    for topic_index in range(k):\n",
    "        descriptors.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "        str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "        print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )\n",
    "    print('________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Я выбираю NMF с K=25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Строим рекомендательную систему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(leaf_size=30, metric='euclidean').fit(W) #euclidean - самая стандартная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NewsRecommender:\n",
    "\n",
    "    def train(self, texts):\n",
    "        model = NMF( init=\"nndsvd\", n_components=25) \n",
    "        W = model.fit_transform(texts)\n",
    "        H = model.components_\n",
    "    \n",
    "    def recommend(self, text_sample, k):\n",
    "        recommendations = []\n",
    "        doc = vectorizer.transform([text_sample])\n",
    "        doc_nmf = model.transform(doc)\n",
    "        indices = neigh.kneighbors(doc_nmf)[1][0]\n",
    "        for ind in indices:\n",
    "            recommendations.append(raw_documents[ind])\n",
    "        return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recom = NewsRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recom.train(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст из новостей BBC-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = recom.recommend('Nokia once dominated the mobile phone market but struggled after the launch of the iPhone a decade ago, and the subsequent release of Googles Android operating system. HMD Global had previously indicated it would release several Nokia-branded Android phones in 2017. It is expected to provide details of at least some of the other launches at another trade show - Barcelonas Mobile World Congress - in February. \"The decision by HMD to launch its first Android smartphone into China is a reflection of the desire to meet the real world needs of consumers in different markets around the world,\" the firm said in a statement. \"With over 552 million smartphone users in China in 2016, a figure that is predicted to grow to more than 593 million users by 2017, it is a strategically important market where premium design and quality is highly valued by consumers.\"', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: sp1marse@kristin (marco seirio)\n",
      "subject: flat globe\n",
      "lines: 13\n",
      "x-newsreader: tin 1.1 pl3\n",
      "\n",
      "\n",
      "does anybody have an algorithm for \"flattening\" out a globe, or any other\n",
      "parametric surface, that is definied parametrically. \n",
      "that is, i would like to take a sheet of paper and a knife and to be\n",
      "able to calculate how i must cut in the paper so i can fold it to a\n",
      "globe (or any other object).\n",
      "\n",
      "\n",
      "      marco seirio - in real life sp1marse@caligula.his.se\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "///////////////////////////\n",
      "from: smb@research.att.com (steven bellovin)\n",
      "subject: re: clipper will corrupt cops (was wh proposal from police point of view)\n",
      "organization: at&t bell laboratories\n",
      "lines: 65\n",
      "\n",
      "in article <1993apr21.041033.16550@news.clarkson.edu>, tuinstra@signal.ece.clarkson.edu.soe (dwight tuinstra) writes:\n",
      "> the clear implication is that there are \"legal\" authorizations other\n",
      "> than a court order.  just how leaky are these?\n",
      "\n",
      "i don't have the wiretap statute handy.  but here's what the law says\n",
      "on pen registers.  this is all from title 18 of the u.s. code.  note\n",
      "how vague s. 3125(a)(1)(b) is....  i haven't had a chance to check\n",
      "out 50 u.s.c. 1801 yet.\n",
      "\n",
      "----\n",
      "\n",
      "18 usc  s. 3121 pen registers (as of 4/93)\n",
      "\n",
      "\n",
      "s. 3121. general prohibition on  pen register  and trap and trace device\n",
      "use; exception\n",
      "\n",
      "   (a) in general. except as provided in this section, no person may\n",
      "install or use a  pen register  or a trap and trace device without first\n",
      "obtaining a court order under section 3123 of this title or under the\n",
      "foreign intelligence surveillance act of 1978 (50 u.s.c. 1801 et seq.).\n",
      "\n",
      ".....\n",
      "\n",
      "s. 3125.  emergency  pen register  and trap and trace device\n",
      "installation\n",
      "\n",
      "   (a) notwithstanding any other provision of this chapter , any\n",
      "investigative or law enforcement officer, specially designated by the\n",
      "attorney general, the deputy attorney general, the associate attorney\n",
      "general, any assistant attorney general, any acting assistant attorney\n",
      "general, or any deputy assistant attorney general, or by the principal\n",
      "prosecuting attorney of any state or subdivision thereof acting pursuant\n",
      "to a statute of that state, who reasonably determines that--\n",
      "\n",
      "   (1) an emergency situation exists that involves--\n",
      "\n",
      "   (a) immediate danger of death or serious bodily injury to any person;\n",
      "or\n",
      "\n",
      "   (b) conspiratorial activities characteristic of organized crime,\n",
      "\n",
      "   that requires the installation and use of a  pen register  or a trap\n",
      "and trace device before an order authorizing such installation and use\n",
      "can, with due diligence, be obtained, and\n",
      "\n",
      "   (2) there are grounds upon which an order could be entered under this\n",
      "chapter to authorize such installation and use \"may have installed and\n",
      "use a  pen register  or trap and trace device if, within forty-eight\n",
      "hours after the installation has occurred, or begins to occur, an order\n",
      "approving the installation or use is issued in accordance with section\n",
      "3123 of this title.\"\n",
      "\n",
      "   (b) in the absence of an authorizing order, such use shall\n",
      "immediately terminate when the information sought is obtained, when the\n",
      "application for the order is denied or when forty-eight hours have\n",
      "lapsed since the installation of the  pen register  or trap and trace\n",
      "device, whichever is earlier.\n",
      "\n",
      "   (c) the knowing installation or use by any investigative or law\n",
      "enforcement officer of a  pen register  or trap and trace device\n",
      "pursuant to subsection (a) without application for the authorizing order\n",
      "within forty-eight hours of the installation shall constitute a\n",
      "violation of this chapter.\n",
      "\n",
      "\n",
      "///////////////////////////\n",
      "from: bakerjp1@netnews.jhuapl.edu (baker john p. pdd x4895 )\n",
      "subject: how does \"differential mode\" gps work???\n",
      "summary: explaination of dgps system\n",
      "keywords: gps, differential, navigation, radio\n",
      "organization: jhu/applied physics laboratory\n",
      "lines: 46\n",
      "\n",
      "\n",
      ">i understand that the new gps boxes now have an option\n",
      ">known as \"differential ready\".  apparently land-based\n",
      ">beacons tranmit gps correction information to your gps \n",
      ">receiver (with differential option installed).\n",
      "\n",
      ">how does this system work?  what frequency is used for\n",
      ">the land-based beacons?\n",
      "\n",
      ">thanks in advance,\n",
      "\n",
      ">charlie thompson\n",
      ">.\n",
      "\n",
      "here's a rough sketch of how the system works.  a reference station\n",
      "with a very exactly known position computes the errors in the\n",
      "incoming gps signals.  these errors are due to several factors\n",
      "including atmospheric distortion, sa (selective availability) time\n",
      "dithering, etc.  the reference unit contains complex computational \n",
      "equipment to \"back out\" the errors in its position (since it knows\n",
      "where it is already).  it then transmits these corrections on a \n",
      "broadcast which is available to any number of relatively local\n",
      "receivers.  if the receivers are nearby (<300km) and are using\n",
      "the same satellites as the reference unit, the errors should be very\n",
      "similar for the reference unit and the receiver unit.  thus, the\n",
      "receiver unit may apply the corrections calculated by the reference\n",
      "unit.\n",
      "\n",
      "the us coast guard is currently (as far as i know) installing a series\n",
      "of coastline transmitters for differential gps.  these stations will\n",
      "use existing radio towers.  i believe the frequency is to be approximately\n",
      "305 khz.  there are many other private corporations offering dgps signals\n",
      "on different frequencies.  for example, pinpoint ((310)-618-7076) offers\n",
      "correction signals and receiver units using an fm broadcast system\n",
      "which has stations all across the us.\n",
      "\n",
      "the correction codes are usually transmitted using the rtcm 104 format.\n",
      "advertised accuracies espouse 1 to 5 meter errors.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "john p. baker                   | my opinions are my own.  i don't know\n",
      "johns hopkins university        | anyone else who wants them, anyway.\n",
      "applied physics laboratory\t|\n",
      "laurel, md  20723               | bakerjp1@aplcomm.jhuapl.edu\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "///////////////////////////\n",
      "from: kmembry@viamar.uucp (kirk membry)\n",
      "subject: re: rumours about 3do ???\n",
      "reply-to: rutgers!viamar!kmembry\n",
      "organization: private system\n",
      "lines: 11\n",
      "\n",
      "read issue #2 of wired magazine.  it has a long article on the \"hype\" of\n",
      "3do.  i've noticed that every article talks with the designers and how\n",
      "\"great\" it is, but never show any pictures of the output (or at least\n",
      "pictures that one can understand)\n",
      "\n",
      "\n",
      "-- \n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "kirk membry                                    \"our age is the age of industry\"\n",
      "rutgers!viamar!kmembry                         - alexander rodchenko\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "///////////////////////////\n",
      "from: ndallen@r-node.hub.org (nigel allen)\n",
      "subject: water supplies vulnerable to milwaukee-type disease outbreak\n",
      "organization: r-node public access unix - 1 416 249 5366\n",
      "lines: 182\n",
      "\n",
      "here is a press release from the natural resources defense council.\n",
      "\n",
      " new data show about 100 major u.s. water supplies vulnerable to\n",
      "milwaukee-type disease outbreak\n",
      " to: national desk, environment writer\n",
      " contact: erik olson or sarah silver, 202-783-7800, both\n",
      "          of the natural resources defense council\n",
      "\n",
      "   washington, april 14  -- internal epa data released\n",
      "today by the natural resources defense council reveals that about\n",
      "100 large water systems -- serving cities from boston to san\n",
      "francisco -- do not filter to remove disease-carrying organisms\n",
      "leaving those communities potentially vulnerable to a disease\n",
      "outbreak similar to the one affecting milwaukee.\n",
      "   the epa list is attached.\n",
      "   \"these internal epa documents reveal that the safety of water\n",
      "supplies in many american cities is threatened by inadequate\n",
      "pollution controls or filtration,\" said erik olson, a senior\n",
      "attorney with nrdc.  \"water contamination isn't just a problem in\n",
      "bangladesh, it's also a problem in bozeman and boston.\"\n",
      "   \"as of june 29, 1993, about 100 large surface water systems on\n",
      "epa's list probably will be breaking the law.  the 1986 safe\n",
      "drinking water act requires all surface water systems to either\n",
      "filter their water or fully protect the rivers or lakes they use\n",
      "from pollution,\" olson continued.  some systems are moving\n",
      "towards eventually implementing filtration systems but are\n",
      "expected to miss the law's deadline.\n",
      "   olson pointed out that the threat of contamination is already\n",
      "a reality in other cities.  a 1991 survey of 66 u.s. surface\n",
      "water systems by water utility scientists found that 87 percent\n",
      "of raw water samples contained the milwaukee organism\n",
      "cryptosporidium, and 81 percent contained a similar parasite\n",
      "called giardia.\n",
      "   adding to the level of concern, a general accounting office\n",
      "study released today by house health and environment subcommittee\n",
      "chairman henry waxman indicates serious deficiencies in the\n",
      "nation's system for conducting and following through on sanitary\n",
      "surveys of water systems.\n",
      "   \"this new information raises a huge warning sign that millions\n",
      "of americans can no longer simply turn on their taps and be\n",
      "assured that their water is safe to drink.  we must immediately\n",
      "put into place programs to protect water sources from\n",
      "contamination and where this is not assured, filtration equipment\n",
      "must be installed to protect the public,\" olson noted.  \"the time\n",
      "has come for many of the nation's water utilities to stop\n",
      "dragging their feet and to aggressively protect their water from\n",
      "contamination; consumers are prepared to pay the modest costs\n",
      "needed to assure their water is safe to drink.\"\n",
      "   nrdc is a national non-profit environmental advocacy organization.\n",
      "\n",
      "   systems epa indicates require filtration and do not adequately\n",
      "protect watersheds\n",
      "\n",
      " connecticut\n",
      "\n",
      " bridgeport            bridgeport hydraulic co.\n",
      "\n",
      " massachusetts\n",
      " boston                h2o resource author (mwra)\n",
      " medford               mwra-medford water dept\n",
      " melrose               mwra-melrose water dept\n",
      " hilton                mwra-hilton water dept\n",
      " needham               mwra-needham water division\n",
      " newtoncenter          mwra-newton water dept.\n",
      " marblehead            mwra-marblehead water dept\n",
      " quincy                mwra-quincy water dept\n",
      " norwood               mwra-norwood water dept\n",
      " framingham            mwra-framingham water div\n",
      " cambridge             mwra-cambridge water dept\n",
      " canton                mwra-canton water div-dpw\n",
      " chelsea               mwra-chelsea water dept\n",
      " everett               mwra-everett water dept\n",
      " lexington             mwra-201 bedford (puo wrks)\n",
      " lynn                  mwra-lynn water & sewer co\n",
      " malden                mwra-malden water division\n",
      " revere                mwra-revere water dept\n",
      " woburn                mwra-woburn water dept\n",
      " swampscott            mwra-swampscott water dept\n",
      " saugus                mwra-saugus water dept\n",
      " somerville            mwra-somerville water dept\n",
      " stoneman              mwra-stoneman water dept\n",
      " brookline             mwra-brookline water dept\n",
      " wakefield             mwra-same as above\n",
      " waltham               mwra-waltham water division\n",
      " watertown             mwra-watertown water division\n",
      " weston                mwra-weston water dept\n",
      " dedham                mwra-dedham-westwood district\n",
      " winchester            mwra-winchester water & sewer\n",
      " winthrop              mwra-winthrop water dept\n",
      " boston                mwra-boston water & sewer co\n",
      " s. hadley             mwra-south hadley fire dist\n",
      " arlington             mwra-arlington water dept\n",
      " belmont               mwra-belmont water dept\n",
      " clinton               mwra-clinton water dept\n",
      " attleboro             attleboro water dept\n",
      " fitchburg             fitchburg water dept\n",
      " northampton           northampton water dept\n",
      " north adams           north adams water dept\n",
      " amherst               amherst water division dpw\n",
      " gardner               gardner water dept\n",
      " worcester             worcester dpw, water oper\n",
      " westboro              westboro water dept\n",
      " southbridge           southbridge water supply co\n",
      " newburyport           newburyport water dept\n",
      " hingham               hingham water co\n",
      " brockton              brockton water dept\n",
      "\n",
      " maine\n",
      " rockland              camden & rockland water co\n",
      " bath                  bath water district\n",
      "\n",
      " new hampshire\n",
      " keene                 city of keene\n",
      " salem                 salem water dept\n",
      "\n",
      " vermont\n",
      " barre city            barre city water system\n",
      " rutland city          rutland city water dept\n",
      "\n",
      " new york\n",
      " glens falls           glens falls city\n",
      " yorktown hts          yorktown water storage & dist\n",
      " rochester             rochester city\n",
      " henrietta             henrietta wd\n",
      " rochester             mcwa upland system\n",
      " rochester             greece consolidated\n",
      " new york              nyc-aquaduct sys (croton)\n",
      " chappaqua             new castle/stanwood wd\n",
      " beacon                beacon city\n",
      " mamaronek             westchester joint water works\n",
      "\n",
      " pennsylvania\n",
      " bethlehem             bethlehem public water sys\n",
      " johnstown             greater johnstown water auth\n",
      " lock haven            city of lock haven-water dept\n",
      " shamokin              roaring creek water comp\n",
      " harrisburg            harrisburg city\n",
      " hazleton              hazleton city water dept\n",
      " wind gap              blue mt consolidated\n",
      " apollo                westmoreland auth\n",
      " fayettville           guilford water auth\n",
      " humlock creek         pg&w-ceasetown reservoir\n",
      " springbrook           pg&w-waters reservoir\n",
      " wilkes barre          pg&w-gardners creek\n",
      " wilkes barre          pg&w-hill creek\n",
      " wilkes barre          pg&w-plymouth relief\n",
      " altoona               altoona city auth\n",
      " tamaqua               tamaqua municipal water\n",
      " waynesboro            waynesboro borough auth\n",
      " pottsville            schuykill co mun auth\n",
      "\n",
      " virginia\n",
      " covington             city of covington\n",
      " fishersville          south river sa dist-acsa\n",
      "\n",
      " south carolina\n",
      " greenville            greenville water sys\n",
      "\n",
      " michigan\n",
      " sault ste marie       sault ste marie\n",
      " marquette             marquette\n",
      "\n",
      " montana\n",
      " butte                 butte water co\n",
      " bozeman               bozeman city\n",
      "\n",
      " california\n",
      " san francisco         city & county of san fran\n",
      "\n",
      " nevada\n",
      " reno                  westpac\n",
      "\n",
      " idaho\n",
      " twin falls            twin falls city\n",
      "\n",
      " washington\n",
      " aberdeen              aberdeen water dept\n",
      " centralia             centralia water dept\n",
      "\n",
      " -30-\n",
      "-- \n",
      "nigel allen, toronto, ontario, canada    ndallen@r-node.hub.org\n",
      "\n",
      "///////////////////////////\n"
     ]
    }
   ],
   "source": [
    "for txt in r:\n",
    "    print(txt)\n",
    "    print('///////////////////////////')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст из обучающего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = raw_documents[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = recom.recommend(text, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: exuptr@exu.ericsson.se (patrick taylor, the sounding board)\n",
      "subject: re: how to the disks copy protected.\n",
      "nntp-posting-host: 138.85.253.85\n",
      "organization: ericsson network systems, inc.\n",
      "x-disclaimer: this article was posted by a user at ericsson.\n",
      "              any opinions expressed are strictly those of the\n",
      "              user and not necessarily those of ericsson.\n",
      "lines: 36\n",
      "\n",
      "in article <1993apr21.131908.29582@uhura.neoucom.edu> wtm@uhura.neoucom.edu (bill mayhew) writes:\n",
      ">from: wtm@uhura.neoucom.edu (bill mayhew)\n",
      ">subject: re: how to the disks copy protected.\n",
      ">date: wed, 21 apr 1993 13:19:08 gmt\n",
      "\n",
      ">write a good manual to go with the software.  the hassle of\n",
      ">photocopying the manual is offset by simplicity of purchasing\n",
      ">the package for only $15.  also, consider offering an inexpensive\n",
      ">but attractive perc for registered users.  for instance, a coffee\n",
      ">mug.  you could produce and mail the incentive for a couple of\n",
      ">dollars, so consider pricing the product at $17.95.\n",
      "\n",
      "or, _documentation_ for the program ;-).  a lot of shareware out there is \n",
      "very similar in the approach - send in your money, and you get \n",
      "documentation, and a free upgrade to the latest version.  perhaps even \n",
      "support of some small degree.  whatever you want to offer that is \"better\" \n",
      "than the circulating version.\n",
      "\n",
      ">you're lucky if only 20% of the instances of your program in use\n",
      ">are non-licensed users.\n",
      "\n",
      "figure about 50%, as i have seen.\n",
      "\n",
      ">the best approach is to estimate your loss and accomodate that into\n",
      ">your price structure.  sure it hurts legitimate users, but too bad.\n",
      "\n",
      "it doesn't really hurt legit users.  shareware is still much cheaper than \n",
      "the alternatives.\n",
      "\n",
      " ----------------------------------------------------------------------------\n",
      " ---------visit the sounding board bbs +1 214 596 2915, a wildcat! bbs-------\n",
      "\n",
      " obdis: all opinions are specifically disclaimed. no one is responsible.\n",
      "\n",
      "    patrick taylor, ericsson network systems  thx-1138\n",
      "    exuptr@exu.ericsson.se                    \"don't let the .se fool you\"\n",
      "\n",
      "//////////////////////\n",
      "from: hans meyer <hmmeyer@silver.ucs.indiana.edu>\n",
      "subject: logitech scanman 256\n",
      "organization: indiana university\n",
      "lines: 15\n",
      "\n",
      "i would like to sell my logitech hand-held 256 gray scale scanner. i\n",
      "originally bought it as a toy and have no practical use for it. hardly\n",
      "ever used it.\n",
      "\n",
      "package includes:\n",
      "-board\n",
      "-scan-mate software\n",
      "-ansel image editing software\n",
      "-all original manuals, box, etc.\n",
      "\n",
      "originally bought for $350 in jan '92.\n",
      "selling for $150.\n",
      "\n",
      "if interested, let me know.\n",
      "-hans meyer\n",
      "\n",
      "//////////////////////\n",
      "from: shanlps@ducvax.auburn.edu\n",
      "subject: tv reception:  heelllppp!!!\n",
      "lines: 18\n",
      "nntp-posting-host: ducvax\n",
      "organization: auburn university, al\n",
      "lines: 18\n",
      "\n",
      "hello,\n",
      "\n",
      "i just canceled my support of the cable regime and i would like to at\n",
      "least pick up the 3 networks and nbc.  :)   i do not have tons of\n",
      "money nor even a few pounds so what i am looking for is the best solution\n",
      "for reception for under 100 dollars.  i have seen modules that you plug\n",
      "into your wall outlet that \"supposedly\" make your entire house an\n",
      "antenna.  i have to admit, even with my limited knowledge of wavelength\n",
      "and aerial reception, this seems dubious in its claims for \"excellent\n",
      "reception\" at best.  i'll try anything, though, if it works.  i am in\n",
      "a non-mountainous area, approximately 50 miles from the transmitting stations\n",
      "which are pretty large (montgomery alabama pop. 200,000) and colombus goergia,\n",
      "pop. 100,000+.  any recommendations of products, brand-names, prices and \n",
      "company info (catalog ordering numbers, addresses etc.)?  thank you in\n",
      "advance.\n",
      "\n",
      "paul sylvester shanley   pshanley@humsci.auburn.edu   voice 205 887 7440\n",
      "\n",
      "\n",
      "//////////////////////\n",
      "from: harter5255@iscsvax.uni.edu\n",
      "subject: help on hand scanners wanted\n",
      "organization: university of northern iowa\n",
      "lines: 30\n",
      "\n",
      "fellow netters,\n",
      "\n",
      "i'm in the market for a hand scanner.  however, i don't know anyone who has\n",
      "one.  i have my eye on two choices.\n",
      "\n",
      "dexxa:  this scanner is available at wal-mart for $90.  it includes grayworks\n",
      "software and provides 400 dpi and 32 grayscales (i think).  the ocr software\n",
      "catchword is available through mail-order for about $90 also.\n",
      "\n",
      "mustek:  (gray artist for windows)  this scanner offers 256 grayscales\n",
      "(according to cad & graphics) and 800 dpi.  it is available for $169\n",
      "mail-order and comes with perceive ocr and picture publisher le.\n",
      "\n",
      "i am also looking at a genius hand scanner (b105) from cad & graphics.  it\n",
      "is basically the same as the mustek scanner except for the resolution (400\n",
      "dpi) and price ($149).  \n",
      "\n",
      "basically, i would like recommendations on which to buy.  i have heard that\n",
      "logitech makes the best and manufactures dexxa scanners.  but which one is the\n",
      "best buy?  would 800 dpi really be helpful (output would be no better than hp\n",
      "laserjet iii or canon bj-200 - 300x300 to 360x360)?  i am leaning toward the\n",
      "mustek because it offers the most features and is in the middle in terms of\n",
      "prices.  which should i buy?\n",
      "\n",
      "if you have a hand scanner, please let me know whether or not you would\n",
      "recommend it.  also, if you know of another scanner within the price range\n",
      "(under $225) that would be a better deal, please e-mail me.  any and all help\n",
      "would be greatly appreciated.\n",
      "\n",
      "- kevin harter\n",
      "\n",
      "//////////////////////\n",
      "from: dsnyder@falcon.aamrl.wpafb.af.mil\n",
      "subject: re: real time graphics??\n",
      "distribution: na\n",
      "organization: usaf al/cfh, wpafb, dayton, oh\n",
      "lines: 27\n",
      "\n",
      "in article <1993apr5.114428.2061@falcon.aamrl.wpafb.af.mil>, dsnyder@falcon.aamrl.wpafb.af.mil writes:\n",
      "> in article <c4va9r.kk7@taurus.cs.nps.navy.mil>, stockel@oahu.oc.nps.navy.mil (jim stockel) writes:\n",
      ">> hi,\n",
      ">> \n",
      ">> \n",
      "\n",
      "\n",
      " opps!  typed in the phone numbers wrong.  here are the correct numbers.\n",
      "\n",
      "> \n",
      ">   for a commerical package try wave from  precision visuals\n",
      "\n",
      "\n",
      "                                            303-530-9000\n",
      "\n",
      "> \n",
      ">   for a free package try khoros from university of new mexico\n",
      "\n",
      "\n",
      "                                       505-277-6563\n",
      "\n",
      "\n",
      ">                                    ftp from\n",
      ">                               ptrg.eece.unm.edu\n",
      "> \n",
      ">     login in anonyomus or ftp  with a valid email address as the password\n",
      ">                cd /pub/khoros/release\n",
      "\n",
      "//////////////////////\n"
     ]
    }
   ],
   "source": [
    "for i in m:\n",
    "    print(i)\n",
    "    print('//////////////////////')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
